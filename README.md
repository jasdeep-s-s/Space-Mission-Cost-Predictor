<div align="center">

# ğŸš€ AI Space Mission Cost Predictor

### *Predicting the Future of Space Exploration with Machine Learning*

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![Jupyter](https://img.shields.io/badge/Jupyter-Notebook-orange.svg)](https://jupyter.org/)
[![scikit-learn](https://img.shields.io/badge/scikit--learn-ML-yellow.svg)](https://scikit-learn.org/)
[![XGBoost](https://img.shields.io/badge/XGBoost-Optimized-green.svg)](https://xgboost.readthedocs.io/)
[![License: MIT](https://img.shields.io/badge/License-MIT-purple.svg)](LICENSE)

[Overview](#-overview) â€¢ [Features](#-key-features) â€¢ [Model Architecture](#-model-architecture) â€¢ [Installation](#-quick-start) â€¢ [Results](#-performance-metrics) â€¢ [Contributing](#-contributing)

---

</div>

## ğŸŒŒ Overview

Whether it's SpaceX's revolutionary Starship catching system or the democratization of space travel, humanity is entering an era where outer space exploration missions are becoming increasingly frequent and accessible. Companies like SpaceX and Blue Origin are working to make space travel more affordable ğŸ’¸ over time.

This **AI-powered cost predictor** leverages machine learning to forecast space mission costs based on historical mission data, achieving **~90% accuracy** on test data. The model helps mission planners estimate budgets and optimize resource allocation for future space exploration endeavors.

<div align="center">

### ğŸ“Š Quick Stats

| Metric | Value |
|--------|-------|
| **Dataset Size** | 500 missions |
| **Best Model** | XGBoost Regressor |
| **RÂ² Score** | ~91% |
| **MAPE** | ~16.7% |
| **Features Used** | 15 engineered features |

</div>

---

## âœ¨ Key Features

<table>
<tr>
<td width="50%">

### ğŸ¯ Model Capabilities
- ğŸ’° **Cost Prediction**: Accurate forecasting for space mission budgets
- ğŸ“ˆ **Feature Analysis**: Correlation plots and feature importance visualization
- ğŸ” **Multi-Model Approach**: Decision Tree â†’ Random Forest â†’ XGBoost progression
- âš–ï¸ **Regularization**: Ridge regression to handle overfitting

</td>
<td width="50%">

### ğŸ› ï¸ Technical Features
- ğŸ§ª **Feature Engineering**: Time Efficiency, Energy Demand, Target Distance
- ğŸ“Š **Data Scaling**: StandardScaler normalization
- ğŸ² **Reproducible Results**: Fixed random state (42)
- ğŸ“‰ **Overfitting Control**: L2 regularization with XGBoost

</td>
</tr>
</table>

---

## ğŸ—ï¸ Model Architecture

### ğŸ“‹ Pipeline Overview

```mermaid
graph LR
    A[Raw Dataset<br/>500 missions] --> B[Feature Engineering]
    B --> C[Categorical Encoding]
    C --> D[StandardScaler]
    D --> E[Train/Test Split<br/>80/20]
    E --> F[Model Training]
    F --> G[Decision Tree]
    F --> H[Random Forest]
    F --> I[XGBoost]
    G --> J[Evaluation]
    H --> J
    I --> J
    J --> K[Best Model<br/>RÂ²: 91%]
```

### ğŸ”¬ Feature Engineering

The model creates **3 derived features** to capture complex relationships:

| Feature | Formula | Purpose |
|---------|---------|----------|
| **Time Efficiency** | `Scientific Yield / (Mission Duration + Îµ)` | Measures mission productivity |
| **Energy Demand** | `Payload Weight Ã— Distance from Earth` | Quantifies energy requirements |
| **Target Distance** | `Target Name Code Ã— Distance from Earth` | Captures target-specific distance impact |

### ğŸ¤– Model Comparison

<div align="center">

| Model | RÂ² Score | MAPE | RMSE | Key Advantage |
|-------|----------|------|------|---------------|
| **Decision Tree** | ~75% | ~25% | ~70 | Baseline, interpretable |
| **Random Forest** | **87.11%** | 19.76% | 54.85 | Ensemble reduces variance |
| **XGBoost** | **90.97%** | **16.71%** | **~45** | Best performance, regularized |

</div>

### âš™ï¸ XGBoost Hyperparameters

```python
XGBRegressor(
    objective='reg:squarederror',
    max_depth=5,
    learning_rate=0.1,
    n_estimators=100,
    reg_lambda=1.0,        # L2 regularization
    random_state=42
)
```

---

## ğŸ“Š Dataset Information

### ğŸ“ Dataset: `space_missions_dataset.csv`
- **Size**: 500 space missions
- **Features**: 15 columns (after engineering)
- **Target Variable**: Mission Cost (billion USD)

### ğŸ¯ Input Features

<details>
<summary><b>Click to expand feature list</b></summary>

#### Original Features
- ğŸŒ **Distance from Earth** (light-years)
- â±ï¸ **Mission Duration** (years)
- ğŸ”¬ **Scientific Yield** (points)
- ğŸ‘¨â€ğŸš€ **Crew Size**
- â›½ **Fuel Consumption** (tons)
- ğŸ“¦ **Payload Weight** (tons)
- ğŸ“… **Launch Year**

#### Categorical Features (Encoded)
- ğŸ¯ Target Type Code
- ğŸš€ Mission Type Code
- ğŸ›¸ Launch Vehicle Code
- ğŸª Target Name Code
- ğŸ†” Mission ID Code

#### Engineered Features
- âš¡ Time Efficiency
- ğŸ”‹ Energy Demand
- ğŸ“ Target Distance

</details>

---

## ğŸš€ Quick Start

### ğŸ“¦ Installation

```bash
# Clone the repository
git clone https://github.com/yourusername/Space-Mission-Cost-Predictor.git
cd Space-Mission-Cost-Predictor

# Install required packages
pip install pandas seaborn matplotlib numpy scikit-learn xgboost jupyter
```

### â–¶ï¸ Running the Notebooks

```bash
# Launch Jupyter Notebook
jupyter notebook

# Or use Jupyter Lab
jupyter lab
```

Then open either:
- **`SMC_Predictor.ipynb`** - Original implementation
- **`SMC_Predictor_Regularized.ipynb`** - Regularized version with Ridge regression

### ğŸ“ Usage

1. **Run cells sequentially** from top to bottom
2. The notebook will:
   - Load and explore the dataset
   - Engineer features
   - Train multiple models
   - Visualize feature importance
   - Display performance metrics

---

## ğŸ“ˆ Performance Metrics

### ğŸ† XGBoost Results (Best Model)

<div align="center">

| Metric | Score | Interpretation |
|--------|-------|----------------|
| **RÂ² Score** | 90.97% | Explains 91% of variance in mission costs |
| **MAPE** | 16.71% | Average prediction error of ~17% |
| **Training Split** | 80/20 | 400 training, 100 test missions |

</div>

### ğŸ“Š Evaluation Visualizations

Both notebooks include:
- âœ… Feature importance plots (horizontal bar charts)
- âœ… Correlation heatmaps
- âœ… Pair plots for feature relationships
- âœ… Model performance comparisons

---

## ğŸ¯ Key Challenge: Feature Dominance

### Problem
Certain features dominated predictions, leading to overfitting and poor generalization.

### Solution
Two-pronged approach:
1. **Ridge Regression** (`SMC_Predictor_Regularized.ipynb`) - Reduces influence of dominant features
2. **XGBoost L2 Regularization** (`reg_lambda=1.0`) - Penalizes large coefficients

**Result**: Improved generalization while maintaining high accuracy

---

## ğŸ—‚ï¸ Project Structure

```
Space-Mission-Cost-Predictor/
â”‚
â”œâ”€â”€ ğŸ“Š space_missions_dataset.csv          # Dataset (500 missions)
â”œâ”€â”€ ğŸ““ SMC_Predictor.ipynb                  # Main implementation
â”œâ”€â”€ ğŸ““ SMC_Predictor_Regularized.ipynb      # Regularized version
â”œâ”€â”€ ğŸ“„ README.md                            # This file
â”œâ”€â”€ ğŸ“„ WARP.md                              # Development guide
â””â”€â”€ ğŸ“ .ipynb_checkpoints/                  # Jupyter checkpoints
```

---

## ğŸ› ï¸ Technologies Used

<div align="center">

### Core Libraries

![Python](https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white)
![Jupyter](https://img.shields.io/badge/Jupyter-F37626?style=for-the-badge&logo=jupyter&logoColor=white)
![Pandas](https://img.shields.io/badge/Pandas-150458?style=for-the-badge&logo=pandas&logoColor=white)
![NumPy](https://img.shields.io/badge/NumPy-013243?style=for-the-badge&logo=numpy&logoColor=white)
![scikit-learn](https://img.shields.io/badge/scikit--learn-F7931E?style=for-the-badge&logo=scikit-learn&logoColor=white)

### Visualization & ML

![Matplotlib](https://img.shields.io/badge/Matplotlib-11557c?style=for-the-badge&logo=python&logoColor=white)
![Seaborn](https://img.shields.io/badge/Seaborn-3776AB?style=for-the-badge&logo=python&logoColor=white)
![XGBoost](https://img.shields.io/badge/XGBoost-00758F?style=for-the-badge&logo=xgboost&logoColor=white)

</div>

---

## ğŸ”¬ Reproducibility

To reproduce the exact results:

1. âœ… Use the same library versions (especially `scikit-learn` and `xgboost`)
2. âœ… Maintain `random_state=42` for train/test split
3. âœ… Run all notebook cells sequentially from the beginning
4. âœ… Use the provided hyperparameters for XGBoost

---

## ğŸ¤ Contributing

Contributions are welcome! Here are some ways you can help:

- ğŸ› Report bugs and issues
- ğŸ’¡ Suggest new features or improvements
- ğŸ“– Improve documentation
- ğŸ”§ Submit pull requests

### Development Setup

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

---

## ğŸ“œ License

Distributed under the **MIT License**. See `LICENSE` for more information.

---

## ğŸŒŸ Acknowledgments

- ğŸš€ Inspired by SpaceX, Blue Origin, and the future of accessible space travel
- ğŸ“Š Built with open-source ML libraries
- ğŸŒŒ Dedicated to advancing space exploration through data science

---

<div align="center">

### â­ Star this repository if you found it helpful!

**Made with â¤ï¸ for the future of space exploration**

[â¬† Back to Top](#-ai-space-mission-cost-predictor)

</div>
